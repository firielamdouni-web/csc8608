# CI : Deep Reinforcement Learning

---

## Exercice 1 : Comprendre la Matrice et Instrumenter l'Environnement (Exploration de Gymnasium)


### Question 1.a : 

### Question 1.b : 

#### Rapport de vol obtenu :

![alt text](img/1.png)

#### Animation du vol :

![alt text](random_agent.gif)

#### Analyse :
Un agent est consid√©r√© comme "r√©solvant" l'environnement lorsqu'il atteint un score moyen de **+200 points**.

Mon agent al√©atoire a obtenu un score de **-134.77 points**, ce qui est tr√®s loin de l'objectif (√©cart d'environ 335 points). Cela s'explique par le comportement compl√®tement d√©sordonn√© de l'agent qui :
- Allume les moteurs de fa√ßon al√©atoire sans strat√©gie coh√©rente (13 allumages du moteur principal et 29 allumages des moteurs lat√©raux en seulement 60 frames)
- Ne cherche pas √† stabiliser la descente
- Consomme du carburant de mani√®re inefficace
- Termine syst√©matiquement par un crash (comme l'indique l'issue du vol)

L'agent al√©atoire sert de **baseline** (r√©f√©rence de base), il montre qu'un comportement non-intelligent est totalement incapable de r√©soudre la t√¢che (score n√©gatif), ce qui justifie l'utilisation d'algorithmes d'apprentissage comme PPO pour esp√©rer atteindre le seuil de +200 points.

## Exercice 2 : Entra√Ænement et √âvaluation de l'Agent PPO (Stable Baselines3)

### Question 2.a. 

### Question 2.b. 

#### √âvolution de la r√©compense moyenne pendant l'entra√Ænement

Pendant l'entra√Ænement, j'ai observ√© la m√©trique `ep_rew_mean` (r√©compense moyenne par √©pisode) dans les logs :

- **Au d√©but de l'entra√Ænement** : la valeur √©tait d'environ **-200** (n√©gative, comportement quasi-al√©atoire)
- **√Ä la fin de l'entra√Ænement** (500 000 timesteps) : la valeur a atteint environ **+121** (positive, montrant que l'agent a appris une strat√©gie)

Cette √©volution montre que l'agent a progressivement appris √† maximiser sa r√©compense en adoptant un comportement plus efficace. On note √©galement que l'`ep_len_mean` (dur√©e moyenne des √©pisodes) est de **427 frames**, ce qui indique que l'agent parvient √† maintenir son vol plus longtemps.

#### Rapport de vol PPO obtenu :

![alt text](img/2.png)

#### Animation du vol :

![alt text](trained_ppo_agent.gif)

#### Comparaison avec l'agent al√©atoire :

| M√©trique | Agent al√©atoire | Agent PPO | Am√©lioration |
|----------|-----------------|-----------|--------------|
| Score | -134.77 points | **+238.92** | ‚úÖ **+373.69 points** |
| Issue du vol | CRASH üí• | **ATTERRISSAGE R√âUSSI üéâ** | ‚úÖ |
| Allumages moteur principal | 13 | **174** | Plus d'utilisations (vol plus long) |
| Allumages moteurs lat√©raux | 29 | **136** | Plus d'utilisations (vol plus long) |
| Dur√©e du vol | 60 frames | **354 frames** | **+294 frames** |

#### L'agent a-t-il atteint le seuil de +200 points ?

**Oui, l'agent a largement d√©pass√© le seuil de +200 points** avec un score de **238.92 points**, ce qui signifie qu'il a appris une strat√©gie efficace pour faire atterrir le module lunaire en douceur.

#### Analyse du comportement :
L'agent PPO a d√©velopp√© une strat√©gie beaucoup plus intelligente que l'agent al√©atoire. En observant le GIF, on constate qu'il :

- **Stabilise la descente** en utilisant les moteurs lat√©raux pour corriger l'inclinaison
- **G√®re le carburant** de fa√ßon strat√©gique (174 allumages du moteur principal r√©partis sur 354 frames)
- **Ralentit progressivement** avant le contact avec le sol
- **Atterrit en douceur** sur la plateforme, ce qui lui permet d'obtenir la r√©compense maximale de +100 points √† la derni√®re frame

Le fait que le vol dure **354 frames** (contre seulement 60 pour l'agent al√©atoire) montre que l'agent cherche activement √† prolonger la mission pour assurer un atterrissage contr√¥l√©, plut√¥t que de s'√©craser rapidement.

## Exercice 3 : L'Art du Reward Engineering (Wrappers et Hacking)

### Question 3.a. 

### Question 3.b. 

#### Rapport de vol PPO HACKED obtenu :

![alt text](img/3.png)


#### Animation du vol :

![alt text](hacked_agent.gif)

#### Analyse de la strat√©gie adopt√©e par l'agent :

**Observation du comportement :**
L'agent a adopt√© une strat√©gie radicale, **il n'utilise jamais le moteur principal** (0 allumage), et se contente d'utiliser uniquement les moteurs lat√©raux (37 allumages). Le vol ne dure que 60 frames (comme l'agent al√©atoire) et se termine syst√©matiquement par un crash.

**Explication math√©matique :**

Pendant l'entra√Ænement, j'ai modifi√© la fonction de r√©compense dans le wrapper :
- Normalement : utiliser le moteur principal co√ªte environ -0.3 points
- Avec le wrapper : utiliser le moteur principal co√ªte **-50 points** (p√©nalit√© massive)

Math√©matiquement, l'agent cherche √† maximiser la fonction objectif :
J(Œ∏) = E[‚àë Œ≥^t * r_t]


Face √† cette p√©nalit√©, l'agent a calcul√© que :
- **Option 1** : Utiliser le moteur principal ‚Üí R√©compense = (r√©compense normale - 50) ‚Üí tr√®s n√©gatif
- **Option 2** : Ne jamais utiliser le moteur principal ‚Üí R√©compense = r√©compense normale (d√©j√† faible sans atterrissage)

L'agent a donc choisi l'option 2 comme "moins pire", il pr√©f√®re crash (r√©compense finale -100) plut√¥t que d'accumuler des p√©nalit√©s de -50 √† chaque utilisation du moteur principal.

**Explication logique :**

Du point de vue de l'agent (qui est "paresseux" et pragmatique), la fonction de r√©compense modifi√©e lui apprend que :
> "Allumer le moteur principal est catastrophique, √ßa me co√ªte presque autant qu'un crash imm√©diat !"

L'agent fait donc le raisonnement suivant :
- Si j'allume le moteur principal, je perds 50 points √† chaque fois ‚Üí je vais vite atteindre -200 voire -300 points
- Si je n'allume que les moteurs lat√©raux, je perds moins de points et je peux peut-√™tre prolonger le vol
- Mais sans moteur principal, je ne peux pas ralentir ma descente ‚Üí je finis par crasher

**Pourquoi ce comportement est-il "optimal" selon la fonction modifi√©e ?**

L'agent a trouv√© la faille dans la fonction de r√©compense : puisque le moteur principal est trop p√©nalis√©, la strat√©gie qui maximise l'esp√©rance de r√©compense est simplement... de ne pas l'utiliser ! C'est un exemple classique de **"reward hacking"** : l'agent exploite une faiblesse de la fonction objectif plut√¥t que d'apprendre le comportement souhait√© (atterrir).

**Comparaison avec l'agent normal de l'exercice 2 :**

| Agent | Moteur principal | Moteurs lat√©raux | Score | Issue |
|-------|------------------|------------------|-------|-------|
| PPO normal (ex2) | 174 | 136 | +238.92 | ‚úÖ Atterrissage |
| PPO hacked (ex3) | **0** | 37 | -105.18 | üí• Crash |

Cette diff√©rence spectaculaire montre l'importance cruciale du **reward engineering** : une mauvaise conception de la fonction de r√©compense peut compl√®tement d√©truire l'apprentissage.


## Exercice 4 : Robustesse et Changement de Physique (G√©n√©ralisation OOD)
### Question 4.a. 

### Question 4.b. 

#### Rapport de vol obtenu :

![alt text](img/4.png)

#### Animation du vol :

![alt text](ood_agent.gif)

#### Analyse du comportement :

**L'agent parvient-il √† se poser calmement ?**

Non, l'agent √©choue √† atterrir et termine par un crash, comme l'indique l'issue du vol. Bien que le score (-64.82) soit meilleur que celui de l'agent al√©atoire (-134.77), il reste tr√®s loin du seuil de +200 points atteint dans l'environnement normal.

**Observations du comportement :**

En regardant le GIF, on peut observer que :
- L'agent **utilise √©norm√©ment les moteurs lat√©raux** (301 allumages, contre 136 en environnement normal)
- Le moteur principal est utilis√© **40 fois** (contre 174 en environnement normal)
- La dur√©e du vol (**344 frames**) est similaire √† celle de l'environnement normal (354 frames)
- Le vaisseau semble **"flotter"** et avoir du mal √† contr√¥ler sa descente √† cause de la faible gravit√©
- L'agent **oscille beaucoup** et consomme √©norm√©ment de carburant pour tenter de se stabiliser
- Finalement, il n'arrive pas √† se poser correctement et crash

#### Explication technique de l'√©chec :

Ce ph√©nom√®ne est un exemple classique de **probl√®me de g√©n√©ralisation "Out-of-Distribution" (OOD)**.

**Pourquoi le mod√®le √©choue-t-il ?**

1. **Diff√©rence de dynamique physique** :
   - L'agent a √©t√© entra√Æn√© sur Terre avec une gravit√© de **-10.0**
   - Il est test√© sur la Lune avec une gravit√© de **-2.0** (5 fois plus faible)
   
   La relation entre les actions et leurs effets est compl√®tement diff√©rente :
   - Sur Terre : une pouss√©e du moteur principal compense fortement la gravit√©
   - Sur la Lune : la m√™me pouss√©e est **trop forte** et fait d√©coller le vaisseau

2. **Surapprentissage (overfitting) √† l'environnement d'entra√Ænement** :
   - Le r√©seau de neurones a appris des s√©quences d'actions sp√©cifiques √† la gravit√© terrestre
   - Il n'a jamais vu de situations avec une gravit√© diff√©rente pendant l'entra√Ænement
   - Les √©tats observ√©s (vitesses, positions) sortent de la distribution apprise

3. **Espace d'observation** :
   L'observation de l'agent inclut la vitesse verticale. En gravit√© faible, les vitesses observ√©es ne correspondent plus aux patterns appris pendant l'entra√Ænement.

#### Comparaison avec l'agent normal :

| M√©trique | Agent normal (Terre, -10.0) | Agent OOD (Lune, -2.0) | Diff√©rence |
|----------|------------------------------|------------------------|------------|
| Score | +238.92 | **-64.82** | üìâ **-303.74 points** |
| Issue | ‚úÖ Atterrissage | üí• Crash | ‚ùå |
| Moteur principal | 174 | **40** | Moins utilis√© |
| Moteurs lat√©raux | 136 | **301** | **+165** utilisations |
| Dur√©e du vol | 354 | 344 | Similaire |

**Conclusion :** 
L'agent a compl√®tement √©chou√© √† g√©n√©raliser √† un nouvel environnement, m√™me avec un changement pourtant simple (modification d'un seul param√®tre physique). Cela illustre parfaitement le d√©fi du **Sim-to-Real gap** en robotique, un mod√®le entra√Æn√© en simulation peut √©chouer lamentablement quand on le d√©ploie dans des conditions r√©elles l√©g√®rement diff√©rentes.

## Exercice 5 : Bilan Ing√©nieur : Le d√©fi du Sim-to-Real
### Question 5.a. 

Face au probl√®me du "Sim-to-Real Gap" mis en √©vidence dans l'exercice pr√©c√©dent (√©chec de l'agent face √† un simple changement de gravit√©), voici deux strat√©gies concr√®tes pour rendre l'agent robuste √† diff√©rentes conditions physiques, sans avoir √† entra√Æner un mod√®le par lune :

#### Strat√©gie n¬∞1 : La randomisation de domaine (Domain Randomization)

**Principe :**
Au lieu d'entra√Æner l'agent dans un environnement fixe (gravit√© = -10.0 constante), on modifie l'environnement d'entra√Ænement pour qu'il pr√©sente des variations al√©atoires √† chaque √©pisode.

**Impl√©mentation concr√®te :**
Pendant l'entra√Ænement (dans `train_and_eval_ppo.py`), on peut cr√©er un wrapper qui randomise la gravit√© √† chaque reset :

```python
class RandomGravityWrapper(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)
        self.gravity_range = (-12.0, -1.0)  # Plage de gravit√©s possibles
    
    def reset(self, **kwargs):
        # Choisir une gravit√© al√©atoire √† chaque √©pisode
        new_gravity = np.random.uniform(*self.gravity_range)
        self.env.unwrapped.gravity = new_gravity
        return self.env.reset(**kwargs)
```

**Avantages :** 

- L'agent apprend √† s'adapter √† diff√©rentes conditions
- Il ne peut pas "surapprendre" une gravit√© sp√©cifique
- Solution simple et peu co√ªteuse en calcul

**R√©sultat attendu :**
L'agent devient robuste √† toute gravit√© comprise dans la plage d'entra√Ænement.

#### Strat√©gie n¬∞2 : Ajouter la gravit√© dans l'espace d'observation

**Principe :**

Actuellement, l'agent re√ßoit 8 observations (position, vitesse, angle, etc.) mais ne conna√Æt pas la gravit√© de l'environnement. En lui donnant explicitement cette information, il peut adapter sa strat√©gie en fonction.

**Impl√©mentation concr√®te :**
On cr√©e un wrapper qui ajoute la valeur de la gravit√© aux observations :

```python
class GravityObservationWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        # Nouvel espace d'observation : anciennes observations + 1 valeur pour la gravit√©
        low = np.append(self.observation_space.low, [-12.0])
        high = np.append(self.observation_space.high, [0.0])
        self.observation_space = gym.spaces.Box(low, high)
    
    def observation(self, observation):
        # Ajouter la gravit√© actuelle aux observations
        gravity = np.array([self.env.unwrapped.gravity])
        return np.append(observation, gravity)
```

**Avantages :**

- L'agent apprend une politique conditionn√©e par la gravit√©
- Il peut adapter son comportement en temps r√©el
- Permet de g√©n√©raliser √† des gravit√©s jamais vues

**R√©sultat attendu :**
L'agent peut inf√©rer la strat√©gie appropri√©e en fonction de la gravit√© qu'il "sent".
Note : Cette approche n√©cessite que la gravit√© soit mesurable ou connue dans l'environnement r√©el, ce qui est le cas en pratique (acc√©l√©rom√®tres).

**Combinaison des deux strat√©gies :**
La solution optimale serait de combiner ces deux approches :

- Randomiser la gravit√© pendant l'entra√Ænement
- Donner la valeur de la gravit√© dans l'observation
- Ainsi, l'agent apprend une fonction du type action = f(√©tat, gravit√©) qui g√©n√©ralise bien √† toutes les conditions.

**Pourquoi ces strat√©gies fonctionnent-elles ?**
Ces approches permettent de passer d'un mod√®le sp√©cifique (qui a m√©moris√© des s√©quences d'actions pour un contexte unique) √† un mod√®le g√©n√©rique (qui a appris le v√©ritable objectif : "atterrir en douceur quelles que soient les conditions").

